{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4816 - loss: 0.7144 - val_accuracy: 0.8077 - val_loss: 0.6281\n",
      "Epoch 2/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - accuracy: 0.7949 - loss: 0.6062 - val_accuracy: 0.8269 - val_loss: 0.5546\n",
      "Epoch 3/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - accuracy: 0.8555 - loss: 0.5438 - val_accuracy: 0.9519 - val_loss: 0.4902\n",
      "Epoch 4/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - accuracy: 0.9046 - loss: 0.4860 - val_accuracy: 0.9712 - val_loss: 0.4378\n",
      "Epoch 5/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - accuracy: 0.9361 - loss: 0.4372 - val_accuracy: 0.9712 - val_loss: 0.3930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x399b70c90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text  # ë°˜ë“œì‹œ ì„í¬íŠ¸ ë˜ì–´ì•¼ tokenizerê°€ ì‘ë™í•¨\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CSV ë¡œë“œ\n",
    "df = pd.read_csv(\"./seller.csv\")\n",
    "\n",
    "# ì…ë ¥ í…ìŠ¤íŠ¸ êµ¬ì„±\n",
    "df[\"text\"] = df[\"íšŒì‚¬ëª…\"].astype(str) + \" \" + df[\"ì£¼ì†Œ\"].astype(str)\n",
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"ë¼ë²¨\"].tolist()\n",
    "\n",
    "# í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬\n",
    "X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# BERT ì‚¬ì „ì²˜ë¦¬ê¸° ë° ì¸ì½”ë” ë¡œë“œ\n",
    "bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
    "bert_encoder = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\")\n",
    "\n",
    "# BERT ì„ë² ë”© í•¨ìˆ˜ ì •ì˜\n",
    "def get_bert_embeddings(text_list):\n",
    "    inputs = bert_preprocess(tf.constant(text_list))\n",
    "    outputs = bert_encoder(inputs)\n",
    "    return outputs[\"pooled_output\"].numpy()\n",
    "\n",
    "# ì„ë² ë”© ìƒì„±\n",
    "X_train_embed = get_bert_embeddings(X_train)\n",
    "X_val_embed = get_bert_embeddings(X_val)\n",
    "\n",
    "# âœ… ë¼ë²¨ì„ np.arrayë¡œ ë³€í™˜\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# ê°„ë‹¨í•œ MLP ë¶„ë¥˜ê¸° ëª¨ë¸ êµ¬ì„±\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(768,)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# ëª¨ë¸ ì»´íŒŒì¼ ë° í•™ìŠµ\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train_embed, y_train, validation_data=(X_val_embed, y_val), epochs=5, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4669 - loss: 0.8027 - val_accuracy: 0.8654 - val_loss: 0.6577\n",
      "Epoch 2/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - accuracy: 0.6669 - loss: 0.6503 - val_accuracy: 0.9519 - val_loss: 0.5805\n",
      "Epoch 3/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - accuracy: 0.8476 - loss: 0.5748 - val_accuracy: 0.9663 - val_loss: 0.5156\n",
      "Epoch 4/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - accuracy: 0.9218 - loss: 0.5150 - val_accuracy: 0.9760 - val_loss: 0.4604\n",
      "Epoch 5/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - accuracy: 0.9463 - loss: 0.4640 - val_accuracy: 0.9663 - val_loss: 0.4139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97       118\n",
      "           1       1.00      0.92      0.96        90\n",
      "\n",
      "    accuracy                           0.97       208\n",
      "   macro avg       0.97      0.96      0.97       208\n",
      "weighted avg       0.97      0.97      0.97       208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text  # ë°˜ë“œì‹œ ìˆì–´ì•¼ tokenizer ì‘ë™\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "df = pd.read_csv(\"seller.csv\")\n",
    "df[\"text\"] = df[\"íšŒì‚¬ëª…\"].astype(str) + \" \" + df[\"ì£¼ì†Œ\"].astype(str)\n",
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"ë¼ë²¨\"].tolist()\n",
    "\n",
    "# í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬\n",
    "X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# BERT ë¡œë“œ\n",
    "bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
    "bert_encoder = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\")\n",
    "\n",
    "# BERT ì„ë² ë”© í•¨ìˆ˜\n",
    "def get_bert_embeddings(text_list):\n",
    "    inputs = bert_preprocess(tf.constant(text_list))\n",
    "    outputs = bert_encoder(inputs)\n",
    "    return outputs[\"pooled_output\"].numpy()\n",
    "\n",
    "# ì„ë² ë”© ìƒì„±\n",
    "X_train_embed = get_bert_embeddings(X_train)\n",
    "X_val_embed = get_bert_embeddings(X_val)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì„±\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(768,)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# í•™ìŠµ\n",
    "model.fit(X_train_embed, y_train, validation_data=(X_val_embed, y_val), epochs=5, batch_size=8)\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "model.save(\"foreign_company_classifier.h5\")\n",
    "\n",
    "# ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "def predict_label(texts):\n",
    "    inputs = bert_preprocess(tf.constant(texts))\n",
    "    embeddings = bert_encoder(inputs)[\"pooled_output\"]\n",
    "    return model.predict(embeddings).flatten()\n",
    "\n",
    "# ì˜ˆì¸¡ ë° í‰ê°€\n",
    "y_pred = predict_label(X_val)\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "report = classification_report(y_val, y_pred_label)\n",
    "print(report)\n",
    "\n",
    "# íŒŒì¼ë¡œ ì €ì¥\n",
    "with open(\"classification_report.txt\", \"w\") as f:\n",
    "    f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ ë° ì¡°í•© í™•ì¥\n",
    "df = pd.read_csv(\"seller.csv\")\n",
    "\n",
    "def generate_augmented_text(row):\n",
    "    company = str(row[\"íšŒì‚¬ëª…\"])\n",
    "    ceo = str(row[\"ë²•ì •ëŒ€í‘œì\"])\n",
    "    address = str(row[\"ì£¼ì†Œ\"])\n",
    "    return [\n",
    "        f\"{company} {address}\",\n",
    "        f\"{company} {ceo}\",\n",
    "        f\"{ceo} {address}\",\n",
    "        f\"{company}\",\n",
    "        f\"{ceo}\",\n",
    "        f\"{address}\"\n",
    "    ]\n",
    "\n",
    "augmented_texts = []\n",
    "augmented_labels = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    texts = generate_augmented_text(row)\n",
    "    augmented_texts.extend(texts)\n",
    "    augmented_labels.extend([row[\"ë¼ë²¨\"]] * len(texts))\n",
    "\n",
    "# 2. í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    augmented_texts, augmented_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. BERT ì‚¬ì „ì²˜ë¦¬ ë° ì¸ì½”ë” ë¡œë“œ\n",
    "bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
    "bert_encoder = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\")\n",
    "\n",
    "def get_bert_embeddings(text_list):\n",
    "    inputs = bert_preprocess(tf.constant(text_list))\n",
    "    outputs = bert_encoder(inputs)\n",
    "    return outputs[\"pooled_output\"].numpy()\n",
    "\n",
    "X_train_embed = get_bert_embeddings(X_train)\n",
    "X_val_embed = get_bert_embeddings(X_val)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# 4. ë¶„ë¥˜ê¸° í•™ìŠµ\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(768,)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train_embed, y_train, validation_data=(X_val_embed, y_val), epochs=5, batch_size=16)\n",
    "\n",
    "# 5. ëª¨ë¸ ì €ì¥\n",
    "model.save(\"foreign_company_classifier.h5\")\n",
    "\n",
    "# 6. ì˜ˆì¸¡ ë° í‰ê°€\n",
    "def predict_label(texts):\n",
    "    inputs = bert_preprocess(tf.constant(texts))\n",
    "    embeddings = bert_encoder(inputs)[\"pooled_output\"]\n",
    "    return model.predict(embeddings).flatten()\n",
    "\n",
    "\n",
    "y_pred = predict_label(X_val)\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "report = classification_report(y_val, y_pred_label)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: ìƒì„± ì¤‘...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ëª…\n",
    "files = [\n",
    "    \"X_train_embed.npy\", \"X_train_label.npy\",\n",
    "    \"X_val_embed.npy\", \"X_val_label.npy\"\n",
    "]\n",
    "checkpoint_exists = all(os.path.exists(f) for f in files)\n",
    "\n",
    "if not checkpoint_exists:\n",
    "    print(\"âœ… ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: ìƒì„± ì¤‘...\")\n",
    "\n",
    "    df = pd.read_csv(\"seller.csv\")\n",
    "\n",
    "    def generate_augmented_text(row):\n",
    "        company = str(row[\"íšŒì‚¬ëª…\"])\n",
    "        ceo = str(row[\"ë²•ì •ëŒ€í‘œì\"])\n",
    "        address = str(row[\"ì£¼ì†Œ\"])\n",
    "        return [\n",
    "            f\"{company} {address}\",\n",
    "            f\"{company} {ceo}\",\n",
    "            f\"{ceo} {address}\",\n",
    "            f\"{company}\",\n",
    "            f\"{ceo}\",\n",
    "            f\"{address}\"\n",
    "        ]\n",
    "\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        texts = generate_augmented_text(row)\n",
    "        augmented_texts.extend(texts)\n",
    "        augmented_labels.extend([row[\"ë¼ë²¨\"]] * len(texts))\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        augmented_texts, augmented_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
    "    bert_encoder = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\")\n",
    "\n",
    "    def get_bert_embeddings(text_list):\n",
    "        inputs = bert_preprocess(tf.constant(text_list))\n",
    "        outputs = bert_encoder(inputs)\n",
    "        return outputs[\"pooled_output\"].numpy()\n",
    "\n",
    "    def save_embeddings(texts, labels, prefix):\n",
    "        X = get_bert_embeddings(texts)\n",
    "        y = np.array(labels)\n",
    "        np.save(f\"{prefix}_embed.npy\", X)\n",
    "        np.save(f\"{prefix}_label.npy\", y)\n",
    "\n",
    "    save_embeddings(X_train, y_train, \"X_train\")\n",
    "    save_embeddings(X_val, y_val, \"X_val\")\n",
    "    print(\"âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ.\")\n",
    "else:\n",
    "    print(\"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘...\")\n",
    "\n",
    "# ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "X_train_embed = np.load(\"X_train_embed.npy\")\n",
    "y_train = np.load(\"X_train_label.npy\")\n",
    "X_val_embed = np.load(\"X_val_embed.npy\")\n",
    "y_val = np.load(\"X_val_label.npy\")\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì„± ë° í•™ìŠµ\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(768,)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "model.fit(X_train_embed, y_train,\n",
    "          validation_data=(X_val_embed, y_val),\n",
    "          epochs=5,\n",
    "          batch_size=16)\n",
    "\n",
    "# ì €ì¥\n",
    "model.save(\"foreign_company_classifier.h5\")\n",
    "print(\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: foreign_company_classifier.h5\")\n",
    "\n",
    "# í‰ê°€\n",
    "y_pred = model.predict(X_val_embed).flatten()\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "\n",
    "print(\"ğŸ“Š ì„±ëŠ¥ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(y_val, y_pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check point ì €ì¥ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: ìƒì„± ì¤‘...\n",
      "âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ.\n",
      "[0 0 1 1 1]\n",
      "[1 1 0 1 0]\n",
      "ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n",
      "Epoch 1/5\n",
      "\u001b[1m339/467\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 446us/step - accuracy: 0.5328 - loss: 0.7005\n",
      "Epoch 1: val_accuracy improved from -inf to 0.87366, saving model to checkpoint/bert_epoch01_valacc0.87.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m467/467\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 738us/step - accuracy: 0.5696 - loss: 0.6850 - val_accuracy: 0.8737 - val_loss: 0.5345\n",
      "Epoch 2/5\n",
      "\u001b[1m381/467\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 397us/step - accuracy: 0.8905 - loss: 0.5063\n",
      "Epoch 2: val_accuracy improved from 0.87366 to 0.92666, saving model to checkpoint/bert_epoch02_valacc0.93.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m467/467\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - accuracy: 0.8934 - loss: 0.5005 - val_accuracy: 0.9267 - val_loss: 0.4081\n",
      "Epoch 3/5\n",
      "\u001b[1m383/467\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 394us/step - accuracy: 0.9374 - loss: 0.3915\n",
      "Epoch 3: val_accuracy improved from 0.92666 to 0.95343, saving model to checkpoint/bert_epoch03_valacc0.95.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m467/467\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - accuracy: 0.9375 - loss: 0.3879 - val_accuracy: 0.9534 - val_loss: 0.3250\n",
      "Epoch 4/5\n",
      "\u001b[1m456/467\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 559us/step - accuracy: 0.9495 - loss: 0.3154\n",
      "Epoch 4: val_accuracy improved from 0.95343 to 0.96681, saving model to checkpoint/bert_epoch04_valacc0.97.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m467/467\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - accuracy: 0.9497 - loss: 0.3150 - val_accuracy: 0.9668 - val_loss: 0.2653\n",
      "Epoch 5/5\n",
      "\u001b[1m373/467\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 404us/step - accuracy: 0.9630 - loss: 0.2606\n",
      "Epoch 5: val_accuracy improved from 0.96681 to 0.97270, saving model to checkpoint/bert_epoch05_valacc0.97.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m467/467\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - accuracy: 0.9638 - loss: 0.2586 - val_accuracy: 0.9727 - val_loss: 0.2225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: foreign_company_classifier.h5\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451us/step\n",
      "ğŸ“Š ì„±ëŠ¥ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.98      0.97       938\n",
      "         1.0       0.98      0.97      0.97       930\n",
      "\n",
      "    accuracy                           0.97      1868\n",
      "   macro avg       0.97      0.97      0.97      1868\n",
      "weighted avg       0.97      0.97      0.97      1868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "# ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ëª…\n",
    "files = [\n",
    "    \"X_train_embed.npy\", \"X_train_label.npy\",\n",
    "    \"X_val_embed.npy\", \"X_val_label.npy\"\n",
    "]\n",
    "checkpoint_exists = all(os.path.exists(f) for f in files)\n",
    "\n",
    "if not checkpoint_exists:\n",
    "    print(\"âœ… ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: ìƒì„± ì¤‘...\")\n",
    "\n",
    "    df = pd.read_csv(\"Balanced_Seller_Dataset.csv\")\n",
    "\n",
    "    def generate_augmented_text(row):\n",
    "        company = str(row[\"íšŒì‚¬ëª…\"])\n",
    "        ceo = str(row[\"ë²•ì •ëŒ€í‘œì\"])\n",
    "        address = str(row[\"ì£¼ì†Œ\"])\n",
    "        return [\n",
    "            f\"{company} {address}\",\n",
    "            f\"{company} {ceo}\",\n",
    "            f\"{ceo} {address}\",\n",
    "            f\"{company}\",\n",
    "            f\"{ceo}\",\n",
    "            f\"{address}\"\n",
    "        ]\n",
    "\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        texts = generate_augmented_text(row)\n",
    "        augmented_texts.extend(texts)\n",
    "        augmented_labels.extend([row[\"ë¼ë²¨\"]] * len(texts))\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        augmented_texts, augmented_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
    "    bert_encoder = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\")\n",
    "\n",
    "    def get_bert_embeddings(text_list, batch_size=128):\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(text_list), batch_size):\n",
    "            batch = text_list[i:i+batch_size]\n",
    "            inputs = bert_preprocess(tf.constant(batch))\n",
    "            outputs = bert_encoder(inputs)\n",
    "            all_embeddings.append(outputs[\"pooled_output\"].numpy())\n",
    "            gc.collect()  # ë©”ëª¨ë¦¬ íšŒìˆ˜\n",
    "        return np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "    def save_embeddings(texts, labels, prefix):\n",
    "        X = get_bert_embeddings(texts)\n",
    "        y = np.array(labels)\n",
    "        np.save(f\"{prefix}_embed.npy\", X)\n",
    "        np.save(f\"{prefix}_label.npy\", y)\n",
    "\n",
    "    save_embeddings(X_train, y_train, \"X_train\")\n",
    "    save_embeddings(X_val, y_val, \"X_val\")\n",
    "    print(\"âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ.\")\n",
    "else:\n",
    "    print(\"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘...\")\n",
    "\n",
    "# ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "X_train_embed = np.load(\"X_train_embed.npy\")\n",
    "y_train = np.load(\"X_train_label.npy\")\n",
    "X_val_embed = np.load(\"X_val_embed.npy\")\n",
    "y_val = np.load(\"X_val_label.npy\")\n",
    "\n",
    "print(y_train[:5])\n",
    "print(y_val[:5])\n",
    "\n",
    "# ë¼ë²¨ ì»¬ëŸ¼ëª… ì œê±°\n",
    "if isinstance(y_train[0], str) and y_train[0] == 'ë¼ë²¨':\n",
    "    y_train = y_train[1:]\n",
    "if isinstance(y_val[0], str) and y_val[0] == 'ë¼ë²¨':\n",
    "    y_val = y_val[1:]\n",
    "\n",
    "X_train_embed = X_train_embed.astype('float32')\n",
    "X_val_embed = X_val_embed.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_val = y_val.astype('float32')\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì„±\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(768,)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# ì²´í¬í¬ì¸íŠ¸ ì½œë°± ì¶”ê°€\n",
    "os.makedirs(\"checkpoint\", exist_ok=True)\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=\"checkpoint/bert_epoch{epoch:02d}_valacc{val_accuracy:.2f}.h5\",\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# í•™ìŠµ ë¡œê·¸ ì €ì¥ ì½œë°±\n",
    "log_cb = CSVLogger(\"train_log.csv\")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "print(\"ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "model.fit(\n",
    "    X_train_embed, y_train,\n",
    "    validation_data=(X_val_embed, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    callbacks=[checkpoint_cb, log_cb]\n",
    ")\n",
    "\n",
    "# ìµœì¢… ëª¨ë¸ ì €ì¥\n",
    "model.save(\"foreign_company_classifier.h5\")\n",
    "print(\"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: foreign_company_classifier.h5\")\n",
    "\n",
    "# í‰ê°€\n",
    "y_pred = model.predict(X_val_embed).flatten()\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "\n",
    "print(\"ğŸ“Š ì„±ëŠ¥ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(y_val, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m533/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7683 - loss: 0.5013\n",
      "Epoch 1: val_accuracy improved from -inf to 0.99450, saving model to checkpoint/bert_epoch01_valacc0.99.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7711 - loss: 0.4974 - val_accuracy: 0.9945 - val_loss: 0.1022\n",
      "Epoch 2/5\n",
      "\u001b[1m514/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9860 - loss: 0.0913\n",
      "Epoch 2: val_accuracy improved from 0.99450 to 0.99541, saving model to checkpoint/bert_epoch02_valacc1.00.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9863 - loss: 0.0899 - val_accuracy: 0.9954 - val_loss: 0.0385\n",
      "Epoch 3/5\n",
      "\u001b[1m507/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9959 - loss: 0.0296\n",
      "Epoch 3: val_accuracy did not improve from 0.99541\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9959 - loss: 0.0292 - val_accuracy: 0.9950 - val_loss: 0.0167\n",
      "Epoch 4/5\n",
      "\u001b[1m516/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9985 - loss: 0.0158\n",
      "Epoch 4: val_accuracy did not improve from 0.99541\n",
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0158 - val_accuracy: 0.9945 - val_loss: 0.0191\n",
      "Epoch 5/5\n",
      "\u001b[1m539/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9970 - loss: 0.0114\n",
      "Epoch 5: val_accuracy improved from 0.99541 to 0.99679, saving model to checkpoint/bert_epoch05_valacc1.00.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m546/546\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9970 - loss: 0.0114 - val_accuracy: 0.9968 - val_loss: 0.0104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m69/69\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00       921\n",
      "         1.0       1.00      1.00      1.00      1260\n",
      "\n",
      "    accuracy                           1.00      2181\n",
      "   macro avg       1.00      1.00      1.00      2181\n",
      "weighted avg       1.00      1.00      1.00      2181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_csv(\"Balanced_Seller_Dataset.csv\")  # â† ë³‘í•©ëœ ìµœì¢… CSV ì‚¬ìš©\n",
    "\n",
    "# ì¦ê°• í•¨ìˆ˜\n",
    "def generate_augmented_text(row):\n",
    "    company = str(row[\"íšŒì‚¬ëª…\"])\n",
    "    ceo = str(row[\"ë²•ì •ëŒ€í‘œì\"])\n",
    "    address = str(row[\"ì£¼ì†Œ\"])\n",
    "    return [\n",
    "        f\"This company, {company}, is located at {address}.\",\n",
    "        f\"{ceo} is the CEO of {company}.\",\n",
    "        f\"{company} is operated by {ceo} and based at {address}.\",\n",
    "        f\"Seller: {company}, CEO: {ceo}.\",\n",
    "        f\"{company} sells from {address}.\",\n",
    "        f\"{address} is the registered address of {company}.\"\n",
    "    ]\n",
    "\n",
    "# ì¦ê°• ì ìš©\n",
    "texts, labels = [], []\n",
    "for _, row in df.iterrows():\n",
    "    aug = generate_augmented_text(row)\n",
    "    texts.extend(aug)\n",
    "    labels.extend([row[\"ë¼ë²¨\"]] * len(aug))\n",
    "\n",
    "# Train/Val Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# BERT ëª¨ë¸\n",
    "bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
    "bert_encoder = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\")\n",
    "\n",
    "def get_bert_embeddings(text_list, batch_size=128):\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch = text_list[i:i+batch_size]\n",
    "        inputs = bert_preprocess(tf.constant(batch))\n",
    "        outputs = bert_encoder(inputs)\n",
    "        all_embeddings.append(outputs[\"pooled_output\"].numpy())\n",
    "        gc.collect()\n",
    "    return np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "# ì„ë² ë”© ìƒì„±\n",
    "X_train_embed = get_bert_embeddings(X_train)\n",
    "X_val_embed = get_bert_embeddings(X_val)\n",
    "y_train = np.array(y_train).astype(\"float32\")\n",
    "y_val = np.array(y_val).astype(\"float32\")\n",
    "\n",
    "# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜\n",
    "weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = dict(enumerate(weights))\n",
    "\n",
    "# ì‹¬ì¸µ ëª¨ë¸ ì •ì˜\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(768,)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# ì½œë°± ì„¤ì •\n",
    "os.makedirs(\"checkpoint\", exist_ok=True)\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=\"checkpoint/bert_epoch{epoch:02d}_valacc{val_accuracy:.2f}.h5\",\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    verbose=1\n",
    ")\n",
    "log_cb = CSVLogger(\"train_log.csv\")\n",
    "\n",
    "# í•™ìŠµ ì‹œì‘\n",
    "model.fit(\n",
    "    X_train_embed, y_train,\n",
    "    validation_data=(X_val_embed, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    callbacks=[checkpoint_cb, log_cb],\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "# ì €ì¥\n",
    "model.save(\"foreign_company_classifier_v2.h5\")\n",
    "\n",
    "# í‰ê°€\n",
    "y_pred = model.predict(X_val_embed).flatten()\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "print(classification_report(y_val, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Available devices:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"ğŸ§  Available devices:\")\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ BERT ì„ë² ë”© ìƒì„± ì¤‘...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# âœ… 1. CSV ë³‘í•© (ê¹¨ì§„ ì¤„ ë¬´ì‹œ)\n",
    "df_main = pd.read_csv(\"seller_eng.csv\")\n",
    "df_new = pd.read_csv(\"seller_eng.csv\", on_bad_lines=\"skip\")  # <- í•µì‹¬\n",
    "df = pd.concat([df_main, df_new], ignore_index=True).drop_duplicates()\n",
    "\n",
    "# âœ… 2. í…ìŠ¤íŠ¸ ì¦ê°•\n",
    "def generate_augmented_text(row):\n",
    "    company = str(row[\"íšŒì‚¬ëª…\"])\n",
    "    ceo = str(row[\"ë²•ì •ëŒ€í‘œì\"])\n",
    "    address = str(row[\"ì£¼ì†Œ\"])\n",
    "    return [\n",
    "        f\"{company} {address}\",\n",
    "        f\"{company} {ceo}\",\n",
    "        f\"{ceo} {address}\",\n",
    "        f\"{company}\",\n",
    "        f\"{ceo}\",\n",
    "        f\"{address}\"\n",
    "    ]\n",
    "\n",
    "augmented_texts, augmented_labels = [], []\n",
    "for _, row in df.iterrows():\n",
    "    texts = generate_augmented_text(row)\n",
    "    augmented_texts.extend(texts)\n",
    "    augmented_labels.extend([row[\"ë¼ë²¨\"]] * len(texts))\n",
    "\n",
    "# âœ… 3. í•™ìŠµ/ê²€ì¦ ë¶„í• \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    augmented_texts, augmented_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# âœ… 4. BERT ë¡œë“œ ë° ì„ë² ë”© ìƒì„±\n",
    "bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
    "bert_encoder = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\")\n",
    "\n",
    "def get_bert_embeddings(text_list):\n",
    "    inputs = bert_preprocess(tf.constant(text_list))\n",
    "    outputs = bert_encoder(inputs)\n",
    "    return outputs[\"pooled_output\"].numpy()\n",
    "\n",
    "print(\"ğŸ“¦ BERT ì„ë² ë”© ìƒì„± ì¤‘...\")\n",
    "X_train_embed = get_bert_embeddings(X_train)\n",
    "X_val_embed = get_bert_embeddings(X_val)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# âœ… 5. ëª¨ë¸ êµ¬ì„± ë° í•™ìŠµ\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(768,)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "model.fit(X_train_embed, y_train,\n",
    "          validation_data=(X_val_embed, y_val),\n",
    "          epochs=5,\n",
    "          batch_size=16)\n",
    "\n",
    "# âœ… 6. ëª¨ë¸ ì €ì¥\n",
    "model.save(\"foreign_company_classifier.h5\")\n",
    "print(\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "# âœ… 7. í‰ê°€\n",
    "y_pred = model.predict(X_val_embed).flatten()\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "print(\"ğŸ“Š í‰ê°€ ê²°ê³¼:\")\n",
    "print(classification_report(y_val, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ğŸ”¸ 1. ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ\n",
    "MODEL_PATH = \"foreign_company_classifier.h5\"\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"{MODEL_PATH} not found. Run initial training first.\")\n",
    "\n",
    "print(\"âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "# ğŸ”¸ 2. ì¶”ê°€ ë°ì´í„° ë¡œë“œ\n",
    "print(\"ğŸ“¦ ì¶”ê°€ CSV ë¡œë”© ì¤‘...\")\n",
    "df_additional = pd.read_csv(\"seller_eng.csv\", on_bad_lines=\"skip\").drop_duplicates()\n",
    "\n",
    "def generate_augmented_text(row):\n",
    "    company = str(row[\"íšŒì‚¬ëª…\"])\n",
    "    ceo = str(row[\"ë²•ì •ëŒ€í‘œì\"])\n",
    "    address = str(row[\"ì£¼ì†Œ\"])\n",
    "    return [\n",
    "        f\"{company} {address}\",\n",
    "        f\"{company} {ceo}\",\n",
    "        f\"{ceo} {address}\",\n",
    "        f\"{company}\",\n",
    "        f\"{ceo}\",\n",
    "        f\"{address}\"\n",
    "    ]\n",
    "\n",
    "# ğŸ”¸ 3. í…ìŠ¤íŠ¸ ì¦ê°•\n",
    "aug_texts, aug_labels = [], []\n",
    "for _, row in df_additional.iterrows():\n",
    "    texts = generate_augmented_text(row)\n",
    "    aug_texts.extend(texts)\n",
    "    aug_labels.extend([row[\"ë¼ë²¨\"]] * len(texts))\n",
    "\n",
    "# ğŸ”¸ 4. í•™ìŠµ/ê²€ì¦ ë¶„í• \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    aug_texts, aug_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ğŸ”¸ 5. ì„ë² ë”© ìƒì„±\n",
    "print(\"ğŸ”„ BERT ì „ì²˜ë¦¬ ë° ì„ë² ë”© ì¤‘...\")\n",
    "bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
    "bert_encoder = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\")\n",
    "\n",
    "def get_bert_embeddings(text_list):\n",
    "    inputs = bert_preprocess(tf.constant(text_list))\n",
    "    outputs = bert_encoder(inputs)\n",
    "    return outputs[\"pooled_output\"].numpy()\n",
    "\n",
    "X_train_embed = get_bert_embeddings(X_train)\n",
    "X_val_embed = get_bert_embeddings(X_val)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# ğŸ”¸ 6. ëª¨ë¸ ê³„ì† í•™ìŠµ\n",
    "print(\"ğŸš€ ëª¨ë¸ ì¶”ê°€ í•™ìŠµ ì¤‘...\")\n",
    "model.fit(\n",
    "    X_train_embed, y_train,\n",
    "    validation_data=(X_val_embed, y_val),\n",
    "    epochs=3,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# ğŸ”¸ 7. ì €ì¥\n",
    "model.save(\"foreign_company_classifier.h5\")\n",
    "print(\"âœ… ëª¨ë¸ ì¬ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "# ğŸ”¸ 8. ì„±ëŠ¥ í‰ê°€\n",
    "y_pred = model.predict(X_val_embed).flatten()\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "\n",
    "print(\"ğŸ“Š ì¶”ê°€ í•™ìŠµ í‰ê°€ ê²°ê³¼:\")\n",
    "print(classification_report(y_val, y_pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¶”ê°€í•™ìŠµ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
