{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4816 - loss: 0.7144 - val_accuracy: 0.8077 - val_loss: 0.6281\n",
      "Epoch 2/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - accuracy: 0.7949 - loss: 0.6062 - val_accuracy: 0.8269 - val_loss: 0.5546\n",
      "Epoch 3/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - accuracy: 0.8555 - loss: 0.5438 - val_accuracy: 0.9519 - val_loss: 0.4902\n",
      "Epoch 4/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - accuracy: 0.9046 - loss: 0.4860 - val_accuracy: 0.9712 - val_loss: 0.4378\n",
      "Epoch 5/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - accuracy: 0.9361 - loss: 0.4372 - val_accuracy: 0.9712 - val_loss: 0.3930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x399b70c90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text  # 반드시 임포트 되어야 tokenizer가 작동함\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CSV 로드\n",
    "df = pd.read_csv(\"./seller.csv\")\n",
    "\n",
    "# 입력 텍스트 구성\n",
    "df[\"text\"] = df[\"회사명\"].astype(str) + \" \" + df[\"주소\"].astype(str)\n",
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"라벨\"].tolist()\n",
    "\n",
    "# 학습/검증 분리\n",
    "X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# BERT 사전처리기 및 인코더 로드\n",
    "bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
    "bert_encoder = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\")\n",
    "\n",
    "# BERT 임베딩 함수 정의\n",
    "def get_bert_embeddings(text_list):\n",
    "    inputs = bert_preprocess(tf.constant(text_list))\n",
    "    outputs = bert_encoder(inputs)\n",
    "    return outputs[\"pooled_output\"].numpy()\n",
    "\n",
    "# 임베딩 생성\n",
    "X_train_embed = get_bert_embeddings(X_train)\n",
    "X_val_embed = get_bert_embeddings(X_val)\n",
    "\n",
    "# ✅ 라벨을 np.array로 변환\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# 간단한 MLP 분류기 모델 구성\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(768,)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# 모델 컴파일 및 학습\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train_embed, y_train, validation_data=(X_val_embed, y_val), epochs=5, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4669 - loss: 0.8027 - val_accuracy: 0.8654 - val_loss: 0.6577\n",
      "Epoch 2/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - accuracy: 0.6669 - loss: 0.6503 - val_accuracy: 0.9519 - val_loss: 0.5805\n",
      "Epoch 3/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - accuracy: 0.8476 - loss: 0.5748 - val_accuracy: 0.9663 - val_loss: 0.5156\n",
      "Epoch 4/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - accuracy: 0.9218 - loss: 0.5150 - val_accuracy: 0.9760 - val_loss: 0.4604\n",
      "Epoch 5/5\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - accuracy: 0.9463 - loss: 0.4640 - val_accuracy: 0.9663 - val_loss: 0.4139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97       118\n",
      "           1       1.00      0.92      0.96        90\n",
      "\n",
      "    accuracy                           0.97       208\n",
      "   macro avg       0.97      0.96      0.97       208\n",
      "weighted avg       0.97      0.97      0.97       208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text  # 반드시 있어야 tokenizer 작동\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv(\"seller.csv\")\n",
    "df[\"text\"] = df[\"회사명\"].astype(str) + \" \" + df[\"주소\"].astype(str)\n",
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"라벨\"].tolist()\n",
    "\n",
    "# 학습/검증 분리\n",
    "X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# BERT 로드\n",
    "bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
    "bert_encoder = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\")\n",
    "\n",
    "# BERT 임베딩 함수\n",
    "def get_bert_embeddings(text_list):\n",
    "    inputs = bert_preprocess(tf.constant(text_list))\n",
    "    outputs = bert_encoder(inputs)\n",
    "    return outputs[\"pooled_output\"].numpy()\n",
    "\n",
    "# 임베딩 생성\n",
    "X_train_embed = get_bert_embeddings(X_train)\n",
    "X_val_embed = get_bert_embeddings(X_val)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# 모델 구성\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(768,)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# 학습\n",
    "model.fit(X_train_embed, y_train, validation_data=(X_val_embed, y_val), epochs=5, batch_size=8)\n",
    "\n",
    "# 모델 저장\n",
    "model.save(\"foreign_company_classifier.h5\")\n",
    "\n",
    "# 예측 함수\n",
    "def predict_label(texts):\n",
    "    inputs = bert_preprocess(tf.constant(texts))\n",
    "    embeddings = bert_encoder(inputs)[\"pooled_output\"]\n",
    "    return model.predict(embeddings).flatten()\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred = predict_label(X_val)\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "report = classification_report(y_val, y_pred_label)\n",
    "print(report)\n",
    "\n",
    "# 파일로 저장\n",
    "with open(\"classification_report.txt\", \"w\") as f:\n",
    "    f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. 데이터 로드 및 조합 확장\n",
    "df = pd.read_csv(\"seller.csv\")\n",
    "\n",
    "def generate_augmented_text(row):\n",
    "    company = str(row[\"회사명\"])\n",
    "    ceo = str(row[\"법정대표자\"])\n",
    "    address = str(row[\"주소\"])\n",
    "    return [\n",
    "        f\"{company} {address}\",\n",
    "        f\"{company} {ceo}\",\n",
    "        f\"{ceo} {address}\",\n",
    "        f\"{company}\",\n",
    "        f\"{ceo}\",\n",
    "        f\"{address}\"\n",
    "    ]\n",
    "\n",
    "augmented_texts = []\n",
    "augmented_labels = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    texts = generate_augmented_text(row)\n",
    "    augmented_texts.extend(texts)\n",
    "    augmented_labels.extend([row[\"라벨\"]] * len(texts))\n",
    "\n",
    "# 2. 학습/검증 분리\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    augmented_texts, augmented_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. BERT 사전처리 및 인코더 로드\n",
    "bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
    "bert_encoder = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\")\n",
    "\n",
    "def get_bert_embeddings(text_list):\n",
    "    inputs = bert_preprocess(tf.constant(text_list))\n",
    "    outputs = bert_encoder(inputs)\n",
    "    return outputs[\"pooled_output\"].numpy()\n",
    "\n",
    "X_train_embed = get_bert_embeddings(X_train)\n",
    "X_val_embed = get_bert_embeddings(X_val)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# 4. 분류기 학습\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(768,)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train_embed, y_train, validation_data=(X_val_embed, y_val), epochs=5, batch_size=16)\n",
    "\n",
    "# 5. 모델 저장\n",
    "model.save(\"foreign_company_classifier.h5\")\n",
    "\n",
    "# 6. 예측 및 평가\n",
    "def predict_label(texts):\n",
    "    inputs = bert_preprocess(tf.constant(texts))\n",
    "    embeddings = bert_encoder(inputs)[\"pooled_output\"]\n",
    "    return model.predict(embeddings).flatten()\n",
    "\n",
    "\n",
    "y_pred = predict_label(X_val)\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "report = classification_report(y_val, y_pred_label)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 체크포인트 없음: 생성 중...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 체크포인트 파일명\n",
    "files = [\n",
    "    \"X_train_embed.npy\", \"X_train_label.npy\",\n",
    "    \"X_val_embed.npy\", \"X_val_label.npy\"\n",
    "]\n",
    "checkpoint_exists = all(os.path.exists(f) for f in files)\n",
    "\n",
    "if not checkpoint_exists:\n",
    "    print(\"✅ 체크포인트 없음: 생성 중...\")\n",
    "\n",
    "    df = pd.read_csv(\"seller.csv\")\n",
    "\n",
    "    def generate_augmented_text(row):\n",
    "        company = str(row[\"회사명\"])\n",
    "        ceo = str(row[\"법정대표자\"])\n",
    "        address = str(row[\"주소\"])\n",
    "        return [\n",
    "            f\"{company} {address}\",\n",
    "            f\"{company} {ceo}\",\n",
    "            f\"{ceo} {address}\",\n",
    "            f\"{company}\",\n",
    "            f\"{ceo}\",\n",
    "            f\"{address}\"\n",
    "        ]\n",
    "\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        texts = generate_augmented_text(row)\n",
    "        augmented_texts.extend(texts)\n",
    "        augmented_labels.extend([row[\"라벨\"]] * len(texts))\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        augmented_texts, augmented_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
    "    bert_encoder = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\")\n",
    "\n",
    "    def get_bert_embeddings(text_list):\n",
    "        inputs = bert_preprocess(tf.constant(text_list))\n",
    "        outputs = bert_encoder(inputs)\n",
    "        return outputs[\"pooled_output\"].numpy()\n",
    "\n",
    "    def save_embeddings(texts, labels, prefix):\n",
    "        X = get_bert_embeddings(texts)\n",
    "        y = np.array(labels)\n",
    "        np.save(f\"{prefix}_embed.npy\", X)\n",
    "        np.save(f\"{prefix}_label.npy\", y)\n",
    "\n",
    "    save_embeddings(X_train, y_train, \"X_train\")\n",
    "    save_embeddings(X_val, y_val, \"X_val\")\n",
    "    print(\"✅ 체크포인트 저장 완료.\")\n",
    "else:\n",
    "    print(\"✅ 체크포인트 로드 중...\")\n",
    "\n",
    "# 불러오기\n",
    "X_train_embed = np.load(\"X_train_embed.npy\")\n",
    "y_train = np.load(\"X_train_label.npy\")\n",
    "X_val_embed = np.load(\"X_val_embed.npy\")\n",
    "y_val = np.load(\"X_val_label.npy\")\n",
    "\n",
    "# 모델 구성 및 학습\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(768,)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"🚀 모델 학습 시작...\")\n",
    "model.fit(X_train_embed, y_train,\n",
    "          validation_data=(X_val_embed, y_val),\n",
    "          epochs=5,\n",
    "          batch_size=16)\n",
    "\n",
    "# 저장\n",
    "model.save(\"foreign_company_classifier.h5\")\n",
    "print(\"✅ 모델 저장 완료: foreign_company_classifier.h5\")\n",
    "\n",
    "# 평가\n",
    "y_pred = model.predict(X_val_embed).flatten()\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "\n",
    "print(\"📊 성능 리포트:\")\n",
    "print(classification_report(y_val, y_pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check point 저장 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 체크포인트 없음: 생성 중...\n",
      "✅ 체크포인트 저장 완료.\n",
      "[0 0 1 1 1]\n",
      "[1 1 0 1 0]\n",
      "🚀 모델 학습 시작...\n",
      "Epoch 1/5\n",
      "\u001b[1m339/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 446us/step - accuracy: 0.5328 - loss: 0.7005\n",
      "Epoch 1: val_accuracy improved from -inf to 0.87366, saving model to checkpoint/bert_epoch01_valacc0.87.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m467/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 738us/step - accuracy: 0.5696 - loss: 0.6850 - val_accuracy: 0.8737 - val_loss: 0.5345\n",
      "Epoch 2/5\n",
      "\u001b[1m381/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 397us/step - accuracy: 0.8905 - loss: 0.5063\n",
      "Epoch 2: val_accuracy improved from 0.87366 to 0.92666, saving model to checkpoint/bert_epoch02_valacc0.93.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m467/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - accuracy: 0.8934 - loss: 0.5005 - val_accuracy: 0.9267 - val_loss: 0.4081\n",
      "Epoch 3/5\n",
      "\u001b[1m383/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 394us/step - accuracy: 0.9374 - loss: 0.3915\n",
      "Epoch 3: val_accuracy improved from 0.92666 to 0.95343, saving model to checkpoint/bert_epoch03_valacc0.95.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m467/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - accuracy: 0.9375 - loss: 0.3879 - val_accuracy: 0.9534 - val_loss: 0.3250\n",
      "Epoch 4/5\n",
      "\u001b[1m456/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 559us/step - accuracy: 0.9495 - loss: 0.3154\n",
      "Epoch 4: val_accuracy improved from 0.95343 to 0.96681, saving model to checkpoint/bert_epoch04_valacc0.97.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m467/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - accuracy: 0.9497 - loss: 0.3150 - val_accuracy: 0.9668 - val_loss: 0.2653\n",
      "Epoch 5/5\n",
      "\u001b[1m373/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 404us/step - accuracy: 0.9630 - loss: 0.2606\n",
      "Epoch 5: val_accuracy improved from 0.96681 to 0.97270, saving model to checkpoint/bert_epoch05_valacc0.97.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m467/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - accuracy: 0.9638 - loss: 0.2586 - val_accuracy: 0.9727 - val_loss: 0.2225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 최종 모델 저장 완료: foreign_company_classifier.h5\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451us/step\n",
      "📊 성능 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.98      0.97       938\n",
      "         1.0       0.98      0.97      0.97       930\n",
      "\n",
      "    accuracy                           0.97      1868\n",
      "   macro avg       0.97      0.97      0.97      1868\n",
      "weighted avg       0.97      0.97      0.97      1868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "# 체크포인트 파일명\n",
    "files = [\n",
    "    \"X_train_embed.npy\", \"X_train_label.npy\",\n",
    "    \"X_val_embed.npy\", \"X_val_label.npy\"\n",
    "]\n",
    "checkpoint_exists = all(os.path.exists(f) for f in files)\n",
    "\n",
    "if not checkpoint_exists:\n",
    "    print(\"✅ 체크포인트 없음: 생성 중...\")\n",
    "\n",
    "    df = pd.read_csv(\"Balanced_Seller_Dataset.csv\")\n",
    "\n",
    "    def generate_augmented_text(row):\n",
    "        company = str(row[\"회사명\"])\n",
    "        ceo = str(row[\"법정대표자\"])\n",
    "        address = str(row[\"주소\"])\n",
    "        return [\n",
    "            f\"{company} {address}\",\n",
    "            f\"{company} {ceo}\",\n",
    "            f\"{ceo} {address}\",\n",
    "            f\"{company}\",\n",
    "            f\"{ceo}\",\n",
    "            f\"{address}\"\n",
    "        ]\n",
    "\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        texts = generate_augmented_text(row)\n",
    "        augmented_texts.extend(texts)\n",
    "        augmented_labels.extend([row[\"라벨\"]] * len(texts))\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        augmented_texts, augmented_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
    "    bert_encoder = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\")\n",
    "\n",
    "    def get_bert_embeddings(text_list, batch_size=128):\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(text_list), batch_size):\n",
    "            batch = text_list[i:i+batch_size]\n",
    "            inputs = bert_preprocess(tf.constant(batch))\n",
    "            outputs = bert_encoder(inputs)\n",
    "            all_embeddings.append(outputs[\"pooled_output\"].numpy())\n",
    "            gc.collect()  # 메모리 회수\n",
    "        return np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "    def save_embeddings(texts, labels, prefix):\n",
    "        X = get_bert_embeddings(texts)\n",
    "        y = np.array(labels)\n",
    "        np.save(f\"{prefix}_embed.npy\", X)\n",
    "        np.save(f\"{prefix}_label.npy\", y)\n",
    "\n",
    "    save_embeddings(X_train, y_train, \"X_train\")\n",
    "    save_embeddings(X_val, y_val, \"X_val\")\n",
    "    print(\"✅ 체크포인트 저장 완료.\")\n",
    "else:\n",
    "    print(\"✅ 체크포인트 로드 중...\")\n",
    "\n",
    "# 불러오기\n",
    "X_train_embed = np.load(\"X_train_embed.npy\")\n",
    "y_train = np.load(\"X_train_label.npy\")\n",
    "X_val_embed = np.load(\"X_val_embed.npy\")\n",
    "y_val = np.load(\"X_val_label.npy\")\n",
    "\n",
    "print(y_train[:5])\n",
    "print(y_val[:5])\n",
    "\n",
    "# 라벨 컬럼명 제거\n",
    "if isinstance(y_train[0], str) and y_train[0] == '라벨':\n",
    "    y_train = y_train[1:]\n",
    "if isinstance(y_val[0], str) and y_val[0] == '라벨':\n",
    "    y_val = y_val[1:]\n",
    "\n",
    "X_train_embed = X_train_embed.astype('float32')\n",
    "X_val_embed = X_val_embed.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_val = y_val.astype('float32')\n",
    "\n",
    "# 모델 구성\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(768,)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# 체크포인트 콜백 추가\n",
    "os.makedirs(\"checkpoint\", exist_ok=True)\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=\"checkpoint/bert_epoch{epoch:02d}_valacc{val_accuracy:.2f}.h5\",\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 학습 로그 저장 콜백\n",
    "log_cb = CSVLogger(\"train_log.csv\")\n",
    "\n",
    "# 모델 학습\n",
    "print(\"🚀 모델 학습 시작...\")\n",
    "model.fit(\n",
    "    X_train_embed, y_train,\n",
    "    validation_data=(X_val_embed, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    callbacks=[checkpoint_cb, log_cb]\n",
    ")\n",
    "\n",
    "# 최종 모델 저장\n",
    "model.save(\"foreign_company_classifier.h5\")\n",
    "print(\"✅ 최종 모델 저장 완료: foreign_company_classifier.h5\")\n",
    "\n",
    "# 평가\n",
    "y_pred = model.predict(X_val_embed).flatten()\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "\n",
    "print(\"📊 성능 리포트:\")\n",
    "print(classification_report(y_val, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m533/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7683 - loss: 0.5013\n",
      "Epoch 1: val_accuracy improved from -inf to 0.99450, saving model to checkpoint/bert_epoch01_valacc0.99.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7711 - loss: 0.4974 - val_accuracy: 0.9945 - val_loss: 0.1022\n",
      "Epoch 2/5\n",
      "\u001b[1m514/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9860 - loss: 0.0913\n",
      "Epoch 2: val_accuracy improved from 0.99450 to 0.99541, saving model to checkpoint/bert_epoch02_valacc1.00.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9863 - loss: 0.0899 - val_accuracy: 0.9954 - val_loss: 0.0385\n",
      "Epoch 3/5\n",
      "\u001b[1m507/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9959 - loss: 0.0296\n",
      "Epoch 3: val_accuracy did not improve from 0.99541\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9959 - loss: 0.0292 - val_accuracy: 0.9950 - val_loss: 0.0167\n",
      "Epoch 4/5\n",
      "\u001b[1m516/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9985 - loss: 0.0158\n",
      "Epoch 4: val_accuracy did not improve from 0.99541\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0158 - val_accuracy: 0.9945 - val_loss: 0.0191\n",
      "Epoch 5/5\n",
      "\u001b[1m539/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9970 - loss: 0.0114\n",
      "Epoch 5: val_accuracy improved from 0.99541 to 0.99679, saving model to checkpoint/bert_epoch05_valacc1.00.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9970 - loss: 0.0114 - val_accuracy: 0.9968 - val_loss: 0.0104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00       921\n",
      "         1.0       1.00      1.00      1.00      1260\n",
      "\n",
      "    accuracy                           1.00      2181\n",
      "   macro avg       1.00      1.00      1.00      2181\n",
      "weighted avg       1.00      1.00      1.00      2181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv(\"Balanced_Seller_Dataset.csv\")  # ← 병합된 최종 CSV 사용\n",
    "\n",
    "# 증강 함수\n",
    "def generate_augmented_text(row):\n",
    "    company = str(row[\"회사명\"])\n",
    "    ceo = str(row[\"법정대표자\"])\n",
    "    address = str(row[\"주소\"])\n",
    "    return [\n",
    "        f\"This company, {company}, is located at {address}.\",\n",
    "        f\"{ceo} is the CEO of {company}.\",\n",
    "        f\"{company} is operated by {ceo} and based at {address}.\",\n",
    "        f\"Seller: {company}, CEO: {ceo}.\",\n",
    "        f\"{company} sells from {address}.\",\n",
    "        f\"{address} is the registered address of {company}.\"\n",
    "    ]\n",
    "\n",
    "# 증강 적용\n",
    "texts, labels = [], []\n",
    "for _, row in df.iterrows():\n",
    "    aug = generate_augmented_text(row)\n",
    "    texts.extend(aug)\n",
    "    labels.extend([row[\"라벨\"]] * len(aug))\n",
    "\n",
    "# Train/Val Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# BERT 모델\n",
    "bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
    "bert_encoder = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\")\n",
    "\n",
    "def get_bert_embeddings(text_list, batch_size=128):\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch = text_list[i:i+batch_size]\n",
    "        inputs = bert_preprocess(tf.constant(batch))\n",
    "        outputs = bert_encoder(inputs)\n",
    "        all_embeddings.append(outputs[\"pooled_output\"].numpy())\n",
    "        gc.collect()\n",
    "    return np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "# 임베딩 생성\n",
    "X_train_embed = get_bert_embeddings(X_train)\n",
    "X_val_embed = get_bert_embeddings(X_val)\n",
    "y_train = np.array(y_train).astype(\"float32\")\n",
    "y_val = np.array(y_val).astype(\"float32\")\n",
    "\n",
    "# 클래스 가중치\n",
    "weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = dict(enumerate(weights))\n",
    "\n",
    "# 심층 모델 정의\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(768,)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# 콜백 설정\n",
    "os.makedirs(\"checkpoint\", exist_ok=True)\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=\"checkpoint/bert_epoch{epoch:02d}_valacc{val_accuracy:.2f}.h5\",\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    verbose=1\n",
    ")\n",
    "log_cb = CSVLogger(\"train_log.csv\")\n",
    "\n",
    "# 학습 시작\n",
    "model.fit(\n",
    "    X_train_embed, y_train,\n",
    "    validation_data=(X_val_embed, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    callbacks=[checkpoint_cb, log_cb],\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "# 저장\n",
    "model.save(\"foreign_company_classifier_v2.h5\")\n",
    "\n",
    "# 평가\n",
    "y_pred = model.predict(X_val_embed).flatten()\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "print(classification_report(y_val, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Available devices:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"🧠 Available devices:\")\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 BERT 임베딩 생성 중...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ✅ 1. CSV 병합 (깨진 줄 무시)\n",
    "df_main = pd.read_csv(\"seller_eng.csv\")\n",
    "df_new = pd.read_csv(\"seller_eng.csv\", on_bad_lines=\"skip\")  # <- 핵심\n",
    "df = pd.concat([df_main, df_new], ignore_index=True).drop_duplicates()\n",
    "\n",
    "# ✅ 2. 텍스트 증강\n",
    "def generate_augmented_text(row):\n",
    "    company = str(row[\"회사명\"])\n",
    "    ceo = str(row[\"법정대표자\"])\n",
    "    address = str(row[\"주소\"])\n",
    "    return [\n",
    "        f\"{company} {address}\",\n",
    "        f\"{company} {ceo}\",\n",
    "        f\"{ceo} {address}\",\n",
    "        f\"{company}\",\n",
    "        f\"{ceo}\",\n",
    "        f\"{address}\"\n",
    "    ]\n",
    "\n",
    "augmented_texts, augmented_labels = [], []\n",
    "for _, row in df.iterrows():\n",
    "    texts = generate_augmented_text(row)\n",
    "    augmented_texts.extend(texts)\n",
    "    augmented_labels.extend([row[\"라벨\"]] * len(texts))\n",
    "\n",
    "# ✅ 3. 학습/검증 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    augmented_texts, augmented_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ 4. BERT 로드 및 임베딩 생성\n",
    "bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
    "bert_encoder = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\")\n",
    "\n",
    "def get_bert_embeddings(text_list):\n",
    "    inputs = bert_preprocess(tf.constant(text_list))\n",
    "    outputs = bert_encoder(inputs)\n",
    "    return outputs[\"pooled_output\"].numpy()\n",
    "\n",
    "print(\"📦 BERT 임베딩 생성 중...\")\n",
    "X_train_embed = get_bert_embeddings(X_train)\n",
    "X_val_embed = get_bert_embeddings(X_val)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# ✅ 5. 모델 구성 및 학습\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(768,)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"🚀 모델 학습 시작...\")\n",
    "model.fit(X_train_embed, y_train,\n",
    "          validation_data=(X_val_embed, y_val),\n",
    "          epochs=5,\n",
    "          batch_size=16)\n",
    "\n",
    "# ✅ 6. 모델 저장\n",
    "model.save(\"foreign_company_classifier.h5\")\n",
    "print(\"✅ 모델 저장 완료\")\n",
    "\n",
    "# ✅ 7. 평가\n",
    "y_pred = model.predict(X_val_embed).flatten()\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "print(\"📊 평가 결과:\")\n",
    "print(classification_report(y_val, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 🔸 1. 기존 모델 로드\n",
    "MODEL_PATH = \"foreign_company_classifier.h5\"\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"{MODEL_PATH} not found. Run initial training first.\")\n",
    "\n",
    "print(\"✅ 기존 모델 로드 중...\")\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "# 🔸 2. 추가 데이터 로드\n",
    "print(\"📦 추가 CSV 로딩 중...\")\n",
    "df_additional = pd.read_csv(\"seller_eng.csv\", on_bad_lines=\"skip\").drop_duplicates()\n",
    "\n",
    "def generate_augmented_text(row):\n",
    "    company = str(row[\"회사명\"])\n",
    "    ceo = str(row[\"법정대표자\"])\n",
    "    address = str(row[\"주소\"])\n",
    "    return [\n",
    "        f\"{company} {address}\",\n",
    "        f\"{company} {ceo}\",\n",
    "        f\"{ceo} {address}\",\n",
    "        f\"{company}\",\n",
    "        f\"{ceo}\",\n",
    "        f\"{address}\"\n",
    "    ]\n",
    "\n",
    "# 🔸 3. 텍스트 증강\n",
    "aug_texts, aug_labels = [], []\n",
    "for _, row in df_additional.iterrows():\n",
    "    texts = generate_augmented_text(row)\n",
    "    aug_texts.extend(texts)\n",
    "    aug_labels.extend([row[\"라벨\"]] * len(texts))\n",
    "\n",
    "# 🔸 4. 학습/검증 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    aug_texts, aug_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 🔸 5. 임베딩 생성\n",
    "print(\"🔄 BERT 전처리 및 임베딩 중...\")\n",
    "bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
    "bert_encoder = hub.load(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\")\n",
    "\n",
    "def get_bert_embeddings(text_list):\n",
    "    inputs = bert_preprocess(tf.constant(text_list))\n",
    "    outputs = bert_encoder(inputs)\n",
    "    return outputs[\"pooled_output\"].numpy()\n",
    "\n",
    "X_train_embed = get_bert_embeddings(X_train)\n",
    "X_val_embed = get_bert_embeddings(X_val)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# 🔸 6. 모델 계속 학습\n",
    "print(\"🚀 모델 추가 학습 중...\")\n",
    "model.fit(\n",
    "    X_train_embed, y_train,\n",
    "    validation_data=(X_val_embed, y_val),\n",
    "    epochs=3,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# 🔸 7. 저장\n",
    "model.save(\"foreign_company_classifier.h5\")\n",
    "print(\"✅ 모델 재저장 완료\")\n",
    "\n",
    "# 🔸 8. 성능 평가\n",
    "y_pred = model.predict(X_val_embed).flatten()\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "\n",
    "print(\"📊 추가 학습 평가 결과:\")\n",
    "print(classification_report(y_val, y_pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추가학습"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
